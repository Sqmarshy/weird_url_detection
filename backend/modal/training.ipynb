{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1c55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "521822b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset_phishing.csv\", encoding='unicode_escape')\n",
    "\n",
    "# Selecting features and target\n",
    "X = data.drop(columns=['url', 'status'])  # Drop non-numerical and target column\n",
    "y = data['status']  # Target column\n",
    "\n",
    "# Encoding the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "random_seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=random_seed, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Creating DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82c414b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingDetectionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PhishingDetectionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, hidden_size // 4)  # Single output for binary classification\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size // 4)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_size // 4, 1)  # Single output for binary classification\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.activation = nn.ReLU()  # Activation for hidden layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # Output logits (raw scores)\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "model = PhishingDetectionModel(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3224e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, save_path):\n",
    "    best_f1 = 0.0  # Track the best F1 score\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        # Training phase\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)  # Raw logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y_batch.unsqueeze(1).float())  # Match output and label shapes\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "            # Apply threshold to get binary predictions\n",
    "            predicted = (probabilities >= 0.5).int()\n",
    "\n",
    "            # Collect predictions and true labels for F1 score and accuracy\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_preds.extend(predicted.numpy().flatten())\n",
    "        \n",
    "        # Calculate F1 score and accuracy for the epoch\n",
    "        epoch_f1 = f1_score(all_labels, all_preds)\n",
    "        epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"F1 Score: {epoch_f1:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "                # Apply threshold to get binary predictions\n",
    "                predicted = (probabilities >= 0.5).int()\n",
    "\n",
    "                # Collect predictions and true labels for F1 score and accuracy\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "                all_preds.extend(predicted.numpy().flatten())\n",
    "        \n",
    "        # Calculate F1 score and accuracy\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Test F1 Score: {f1:.4f}\")        \n",
    "\n",
    "        # Save the model if it performs better\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"New best model saved with F1 Score: {f1:.4f} at epoch {epoch + 1}\")\n",
    "\n",
    "    print(f'Best model saved at epoch {best_epoch + 1} with F1 score of {best_f1:.4f}')\n",
    "\n",
    "# Parameters for training\n",
    "num_epochs = 30\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "save_path = \"best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60eeb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.5838, F1 Score: 0.6283, Accuracy: 0.6966\n",
      "Test Accuracy: 78.57%\n",
      "Test F1 Score: 0.7614\n",
      "New best model saved with F1 Score: 0.7614 at epoch 1\n",
      "Epoch 2/30, Loss: 0.4779, F1 Score: 0.7751, Accuracy: 0.7882\n",
      "Test Accuracy: 80.45%\n",
      "Test F1 Score: 0.7904\n",
      "New best model saved with F1 Score: 0.7904 at epoch 2\n",
      "Epoch 3/30, Loss: 0.4469, F1 Score: 0.7942, Accuracy: 0.8033\n",
      "Test Accuracy: 81.80%\n",
      "Test F1 Score: 0.8049\n",
      "New best model saved with F1 Score: 0.8049 at epoch 3\n",
      "Epoch 4/30, Loss: 0.4374, F1 Score: 0.7971, Accuracy: 0.8053\n",
      "Test Accuracy: 82.85%\n",
      "Test F1 Score: 0.8207\n",
      "New best model saved with F1 Score: 0.8207 at epoch 4\n",
      "Epoch 5/30, Loss: 0.4283, F1 Score: 0.8010, Accuracy: 0.8112\n",
      "Test Accuracy: 81.98%\n",
      "Test F1 Score: 0.8100\n",
      "Epoch 6/30, Loss: 0.4268, F1 Score: 0.8003, Accuracy: 0.8081\n",
      "Test Accuracy: 82.20%\n",
      "Test F1 Score: 0.8127\n",
      "Epoch 7/30, Loss: 0.4156, F1 Score: 0.8065, Accuracy: 0.8151\n",
      "Test Accuracy: 83.07%\n",
      "Test F1 Score: 0.8229\n",
      "New best model saved with F1 Score: 0.8229 at epoch 7\n",
      "Epoch 8/30, Loss: 0.4132, F1 Score: 0.8099, Accuracy: 0.8171\n",
      "Test Accuracy: 83.25%\n",
      "Test F1 Score: 0.8258\n",
      "New best model saved with F1 Score: 0.8258 at epoch 8\n",
      "Epoch 9/30, Loss: 0.4171, F1 Score: 0.8041, Accuracy: 0.8131\n",
      "Test Accuracy: 83.42%\n",
      "Test F1 Score: 0.8241\n",
      "Epoch 10/30, Loss: 0.4117, F1 Score: 0.8140, Accuracy: 0.8205\n",
      "Test Accuracy: 83.25%\n",
      "Test F1 Score: 0.8234\n",
      "Epoch 11/30, Loss: 0.4140, F1 Score: 0.8057, Accuracy: 0.8129\n",
      "Test Accuracy: 83.95%\n",
      "Test F1 Score: 0.8322\n",
      "New best model saved with F1 Score: 0.8322 at epoch 11\n",
      "Epoch 12/30, Loss: 0.4105, F1 Score: 0.8139, Accuracy: 0.8206\n",
      "Test Accuracy: 83.64%\n",
      "Test F1 Score: 0.8309\n",
      "Epoch 13/30, Loss: 0.4104, F1 Score: 0.8156, Accuracy: 0.8220\n",
      "Test Accuracy: 83.51%\n",
      "Test F1 Score: 0.8257\n",
      "Epoch 14/30, Loss: 0.4082, F1 Score: 0.8122, Accuracy: 0.8198\n",
      "Test Accuracy: 83.95%\n",
      "Test F1 Score: 0.8351\n",
      "New best model saved with F1 Score: 0.8351 at epoch 14\n",
      "Epoch 15/30, Loss: 0.4012, F1 Score: 0.8160, Accuracy: 0.8228\n",
      "Test Accuracy: 83.81%\n",
      "Test F1 Score: 0.8309\n",
      "Epoch 16/30, Loss: 0.4007, F1 Score: 0.8178, Accuracy: 0.8237\n",
      "Test Accuracy: 83.20%\n",
      "Test F1 Score: 0.8323\n",
      "Epoch 17/30, Loss: 0.4038, F1 Score: 0.8154, Accuracy: 0.8228\n",
      "Test Accuracy: 84.16%\n",
      "Test F1 Score: 0.8356\n",
      "New best model saved with F1 Score: 0.8356 at epoch 17\n",
      "Epoch 18/30, Loss: 0.3997, F1 Score: 0.8160, Accuracy: 0.8226\n",
      "Test Accuracy: 82.76%\n",
      "Test F1 Score: 0.8214\n",
      "Epoch 19/30, Loss: 0.4032, F1 Score: 0.8123, Accuracy: 0.8196\n",
      "Test Accuracy: 84.38%\n",
      "Test F1 Score: 0.8406\n",
      "New best model saved with F1 Score: 0.8406 at epoch 19\n",
      "Epoch 20/30, Loss: 0.3974, F1 Score: 0.8175, Accuracy: 0.8229\n",
      "Test Accuracy: 84.43%\n",
      "Test F1 Score: 0.8439\n",
      "New best model saved with F1 Score: 0.8439 at epoch 20\n",
      "Epoch 21/30, Loss: 0.4010, F1 Score: 0.8174, Accuracy: 0.8228\n",
      "Test Accuracy: 84.25%\n",
      "Test F1 Score: 0.8355\n",
      "Epoch 22/30, Loss: 0.3951, F1 Score: 0.8143, Accuracy: 0.8210\n",
      "Test Accuracy: 84.12%\n",
      "Test F1 Score: 0.8337\n",
      "Epoch 23/30, Loss: 0.3994, F1 Score: 0.8194, Accuracy: 0.8252\n",
      "Test Accuracy: 83.81%\n",
      "Test F1 Score: 0.8293\n",
      "Epoch 24/30, Loss: 0.3988, F1 Score: 0.8170, Accuracy: 0.8247\n",
      "Test Accuracy: 84.69%\n",
      "Test F1 Score: 0.8411\n",
      "Epoch 25/30, Loss: 0.3949, F1 Score: 0.8186, Accuracy: 0.8238\n",
      "Test Accuracy: 84.03%\n",
      "Test F1 Score: 0.8336\n",
      "Epoch 26/30, Loss: 0.3942, F1 Score: 0.8199, Accuracy: 0.8262\n",
      "Test Accuracy: 83.73%\n",
      "Test F1 Score: 0.8244\n",
      "Epoch 27/30, Loss: 0.3970, F1 Score: 0.8163, Accuracy: 0.8211\n",
      "Test Accuracy: 84.08%\n",
      "Test F1 Score: 0.8372\n",
      "Epoch 28/30, Loss: 0.3942, F1 Score: 0.8224, Accuracy: 0.8270\n",
      "Test Accuracy: 84.25%\n",
      "Test F1 Score: 0.8368\n",
      "Epoch 29/30, Loss: 0.3947, F1 Score: 0.8190, Accuracy: 0.8250\n",
      "Test Accuracy: 84.91%\n",
      "Test F1 Score: 0.8473\n",
      "New best model saved with F1 Score: 0.8473 at epoch 29\n",
      "Epoch 30/30, Loss: 0.3992, F1 Score: 0.8161, Accuracy: 0.8231\n",
      "Test Accuracy: 84.03%\n",
      "Test F1 Score: 0.8343\n",
      "Best model saved at epoch 29 with F1 score of 0.8473\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8234385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.91%\n",
      "Test F1 Score: 0.8473\n",
      "\n",
      "Classification Metrics:\n",
      "True Positives (TP): 957\n",
      "True Negatives (TN): 984\n",
      "False Positives (FP): 159\n",
      "False Negatives (FN): 186\n",
      "Total Samples: 2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leo88\\AppData\\Local\\Temp\\ipykernel_25008\\2939212556.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        probabilities = torch.sigmoid(outputs)  # Shape: (batch_size, 1)\n",
    "        predicted = (probabilities >= 0.5).int()  # Shape: (batch_size, 1)\n",
    "        all_labels.extend(y_batch.numpy())  # True labels\n",
    "        all_preds.extend(predicted.numpy().flatten())  # Predicted binary labels\n",
    "\n",
    "# Calculate F1 score and accuracy\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "if cm.shape == (2, 2):  # Binary classification\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "else:  # Multiclass: calculate separately for each class\n",
    "    TP = cm.diagonal()  # True positives for each class\n",
    "    FP = cm.sum(axis=0) - TP  # False positives for each class\n",
    "    FN = cm.sum(axis=1) - TP  # False negatives for each class\n",
    "    TN = cm.sum() - (FP + FN + TP)  # True negatives for each class\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Metrics:\")\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f'Total Samples: {TP + FP + TN + FN}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
